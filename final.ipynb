{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from obspy import read\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.signal import spectrogram\n",
    "from scipy.ndimage import label\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import os\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicDataProcessor:\n",
    "    def __init__(self, csv_file, mseed_file):\n",
    "        \"\"\"\n",
    "        Initializes the processor with the provided CSV and MiniSEED files.\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file with time and velocity data.\n",
    "            mseed_file (str): Path to the MiniSEED file with frequency data.\n",
    "        \"\"\"\n",
    "        self.csv_file = csv_file\n",
    "        self.mseed_file = mseed_file\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads CSV and MiniSEED data.\"\"\"\n",
    "        try:\n",
    "            # Load CSV data for time and velocity\n",
    "            self.csv_data = pd.read_csv(self.csv_file)\n",
    "            if 'time_rel(sec)' in self.csv_data.columns:\n",
    "                self.csv_data.rename(columns={'time_rel(sec)': 'rel_time(sec)'}, inplace=True)\n",
    "\n",
    "            if 'velocity(m/s)' in self.csv_data.columns:\n",
    "                self.csv_data.rename(columns={'velocity(m/s)': 'velocity(c/s)'}, inplace=True)\n",
    "\n",
    "            if 'time_abs(%Y-%m-%dT%H:%M:%S.%f)' in self.csv_data.columns:\n",
    "                self.csv_data.rename(columns={'time_abs(%Y-%m-%dT%H:%M:%S.%f)': 'time(%Y-%m-%dT%H:%M:%S.%f)'}, inplace=True)\n",
    "            \n",
    "            print(self.csv_data.head())\n",
    "\n",
    "            self.time = self.csv_data['rel_time(sec)'].values\n",
    "            self.velocity = self.csv_data['velocity(c/s)'].values\n",
    "\n",
    "            # Load MiniSEED data for spectrogram\n",
    "            self.mseed_data = read(self.mseed_file)\n",
    "            trace = self.mseed_data[0]  # Assuming single trace\n",
    "            self.sampling_rate = trace.stats.sampling_rate\n",
    "            self.sxx = None\n",
    "            self.frequencies = None\n",
    "            self.calculate_spectrogram(trace.data)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "\n",
    "    def calculate_spectrogram(self, velocity):\n",
    "        \"\"\"Calculates the spectrogram from the velocity data.\"\"\"\n",
    "        self.frequencies, self.time_spec, self.sxx = spectrogram(velocity, fs=self.sampling_rate)\n",
    "\n",
    "    def calculate_power(self):\n",
    "        \"\"\"\n",
    "        Calculate the power of the velocity signal from the spectrogram data.\n",
    "        Returns:\n",
    "            power (ndarray): The power of the signal over time.\n",
    "        \"\"\"\n",
    "        # Power is the square of the spectrogram (sxx) values\n",
    "        power = np.abs(self.sxx) ** 2\n",
    "        return np.mean(power, axis=0)  # Average across frequencies\n",
    "\n",
    "    def detect_power_clusters(self, power_scaled, percentile=90):\n",
    "        \"\"\"Detects clusters in the power data based on a threshold.\"\"\"\n",
    "        power_threshold = np.percentile(power_scaled, percentile)\n",
    "        power_above_threshold = power_scaled > power_threshold\n",
    "        labeled_clusters, num_clusters = label(power_above_threshold)\n",
    "\n",
    "        # Identify the cluster with the maximum total power\n",
    "        max_power_cluster = None\n",
    "        max_cluster_sum = 0\n",
    "\n",
    "        for cluster_id in range(1, num_clusters + 1):\n",
    "            cluster_indices = np.where(labeled_clusters == cluster_id)[0]\n",
    "            cluster_sum = np.sum(power_scaled[cluster_indices])\n",
    "\n",
    "            if cluster_sum > max_cluster_sum:\n",
    "                max_cluster_sum = cluster_sum\n",
    "                max_power_cluster = cluster_indices\n",
    "\n",
    "        arrival_time = self.time_spec[max_power_cluster[0]] if max_power_cluster is not None else None\n",
    "        return max_power_cluster, arrival_time\n",
    "\n",
    "    def smooth_power(self, power, window_len=10):\n",
    "        \"\"\"Smooths the power data using a simple moving average.\"\"\"\n",
    "        return np.convolve(power, np.ones(window_len) / window_len, mode='same')\n",
    "\n",
    "    def plot_results(self, power_scaled, max_power_cluster, arrival_time):\n",
    "        \"\"\"Plots the time series, spectrogram, and power clusters.\"\"\"\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        self.plot_velocity_time_series(arrival_time)\n",
    "        self.plot_spectrogram(arrival_time)\n",
    "        self.plot_power_and_clusters(power_scaled, max_power_cluster, arrival_time)\n",
    "\n",
    "    def plot_velocity_time_series(self, arrival_time):\n",
    "        \"\"\"Plots the velocity time series with the detected trigger time.\"\"\"\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(self.time, self.velocity, label=\"Velocity (c/s)\")\n",
    "        plt.xlabel('Time (sec)')\n",
    "        plt.ylabel('Velocity (c/s)')\n",
    "        plt.title('Velocity Time Series')\n",
    "        if arrival_time:\n",
    "            plt.axvline(arrival_time, color='r', linestyle='--', label='Trigger Time')\n",
    "        plt.legend()\n",
    "        plt.savefig('images/velocity_time_series.png')\n",
    "        plt.close() \n",
    "\n",
    "    def plot_spectrogram(self, arrival_time):\n",
    "        \"\"\"Plots the spectrogram of the velocity data.\"\"\"\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.pcolormesh(self.time_spec, self.frequencies, 10 * np.log10(self.sxx), shading='gouraud')\n",
    "        plt.xlabel('Time (sec)')\n",
    "        plt.ylabel('Frequency (Hz)')\n",
    "        plt.title('Spectrogram (dB)')\n",
    "        if arrival_time:\n",
    "            plt.axvline(arrival_time, color='r', linestyle='--', label='Trigger Time')\n",
    "        plt.colorbar()\n",
    "        plt.savefig('images/spectrogram.png')\n",
    "        plt.close() \n",
    "\n",
    "    def plot_power_and_clusters(self, power_scaled, max_power_cluster, arrival_time):\n",
    "        \"\"\"Plots the scaled power and detected clusters.\"\"\"\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(self.time_spec, power_scaled, label='Smoothed Power', color='b')\n",
    "        if max_power_cluster is not None:\n",
    "            plt.scatter(self.time_spec[max_power_cluster], power_scaled[max_power_cluster], color='r', label='Detected Cluster')\n",
    "        plt.xlabel('Time (sec)')\n",
    "        plt.ylabel('Power')\n",
    "        plt.title('Power Over Time')\n",
    "        if arrival_time:\n",
    "            plt.axvline(arrival_time, color='r', linestyle='--', label='Trigger Time')\n",
    "        plt.legend()\n",
    "        plt.savefig('images/power_clusters.png')\n",
    "        plt.close()\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"Main processing pipeline to load data, compute spectrogram, detect clusters, and plot results.\"\"\"\n",
    "        power_scaled = self.calculate_power()\n",
    "        smoothed_power = self.smooth_power(power_scaled)\n",
    "\n",
    "        # Detect power clusters and arrival time\n",
    "        max_power_cluster, arrival_time = self.detect_power_clusters(smoothed_power)\n",
    "\n",
    "        # Plot the results\n",
    "        self.plot_results(smoothed_power, max_power_cluster, arrival_time)\n",
    "\n",
    "        return arrival_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time(%Y-%m-%dT%H:%M:%S.%f)  rel_time(sec)  velocity(c/s)\n",
      "0  2019-05-23T02:00:00.032000           0.00      -0.000000\n",
      "1  2019-05-23T02:00:00.082000           0.05       0.000199\n",
      "2  2019-05-23T02:00:00.132000           0.10      -0.001630\n",
      "3  2019-05-23T02:00:00.182000           0.15      -0.000875\n",
      "4  2019-05-23T02:00:00.232000           0.20      -0.006137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0y/_qpkhqy57dd3pfbf16f5k2dm0000gn/T/ipykernel_14066/1450868812.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  trigger_data = pd.concat([trigger_data, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    csv_files = []\n",
    "    mseed_files = []\n",
    "   \n",
    "    trigger_data = pd.DataFrame(columns=['filename', 'detected_trigger_time','timestamp'])\n",
    "    \n",
    "    csv_files=[\"data/mars/test/data/XB.ELYSE.02.BHV.2019-05-23HR02_evid0041.csv\"]\n",
    "    mseed_files=[\"data/mars/test/data/XB.ELYSE.02.BHV.2019-05-23HR02_evid0041.mseed\"]\n",
    "\n",
    "    # Process each file pair\n",
    "    for csv_file, mseed_file in zip(csv_files, mseed_files):\n",
    "        processor = SeismicDataProcessor(csv_file,mseed_file)\n",
    "        arrival_time = processor.process_data()\n",
    "        closest_index = (np.abs(processor.time - arrival_time)).argmin()\n",
    "        corresponding_timestamp = processor.csv_data['time(%Y-%m-%dT%H:%M:%S.%f)'].iloc[closest_index]\n",
    "\n",
    "        # Add the result to the trigger data DataFrame using pd.concat()\n",
    "        new_row = pd.DataFrame({\n",
    "            'filename': [os.path.basename(csv_file)],\n",
    "            'detected_trigger_time': [arrival_time],\n",
    "            'timestamp': [corresponding_timestamp]\n",
    "        })\n",
    "\n",
    "        trigger_data = pd.concat([trigger_data, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the detected trigger times to a CSV file\n",
    "    trigger_data.to_csv('final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
